{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"assets/logo.jpg\" alt=\"logo\" width=\"150\" height=\"80\" style=\"float: left; margin-right: 10px;\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<h1><center>Final Deliverable - Artificial Intelligence (Machine Learning)</center></h1>\n",
    "<br><br><br>\n",
    "\n",
    "<center><img src=\"assets/ai.png\" alt=\"map\" width=\"500\" height=\"400\"></center>\n",
    "<br><br><br>\n",
    "\n",
    "Delphin Bonheur NGOGA - Nell PATOU PARVEDY - Clément MARQUES - Joel Stephane YANKAM NGUEGUIM\n"
   ],
   "id": "4a00dc1c10e4698d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "HumanForYou is a pharmaceutical company based in India, employing around 4,000 people. Each year, the company faces a high employee turnover rate of approximately 15%, which causes significant challenges: the projects on which departing employees were working fall behind schedule, damaging the company’s reputation with customers and partners. \n",
    "Furthermore, this turnover requires maintaining a large human resources team to recruit, train, and onboard new employees, leading to increased costs and productivity loss.\n",
    "\n",
    "\n",
    "In this context, management has requested an in-depth analysis of the available data to identify the main factors influencing turnover. The goal is to propose predictive models to guide actions that will motivate employees to stay. This project relies on analyzing several anonymized datasets containing demographic, professional, satisfaction, and performance information about employees.\n",
    "\n",
    "\n",
    "Alongside technical and operational challenges, adopting an ethical approach is essential. The use of artificial intelligence and predictive analytics in human resources raises sensitive issues related to personal data protection, prevention of discrimination, transparency of models, and preservation of individual autonomy. Respecting these principles is crucial to building trust and ensuring that the solution does not negatively impact employees’ quality of work life or fundamental rights.\n"
   ],
   "id": "1c190fdf5ab6e8e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Ethics\n",
    "This study is therefore part of an approach that combines analytical performance with ethical requirements, particularly aligned with the European Commission’s Assessment List for Trustworthy Artificial Intelligence (ALTAI) recommendations.\n",
    "\n",
    "\n",
    "### 2.1 Data supplied\n",
    "The data supplied by HumanForYou comprises several anonymized CSV files, each providing distinct but complementary information for the turnover analysis:\n",
    "\n",
    "<br>\n",
    "•\tGeneral Human Resources Data (general_data.csv)\n",
    "\n",
    "This file contains key demographic and professional attributes for each employee identified by a unique EmployeeID for the year 2015. This comprehensive dataset forms the backbone for understanding employee profiles.\n",
    "\n",
    "<br>\n",
    "•\tLatest Manager Assessment (manager_survey_data.csv)\n",
    "\n",
    "This smaller file contains each employee’s latest evaluation by their manager as of February 2015. It includes ratings for job involvement (from low to very high) and overall performance (from low to beyond expectations). These assessments provide insights into individual engagement and productivity levels as viewed by direct supervisors.\n",
    "\n",
    "<br>\n",
    "•\tWorkplace Quality of Life Survey (employee_survey_data.csv)\n",
    "\n",
    "Collected in June 2015, this survey gathers employee feedback on three dimensions of job satisfaction: satisfaction with the work environment, the job itself, and work-life balance. This dataset helps capture subjective factors influencing morale and retention.\n",
    "\n",
    "<br>\n",
    "•\tWorking Hours Data (in_out_time.zip)\n",
    "\n",
    "Two files within this archive record employee arrival and departure times daily throughout 2015. This time clock data allows analysis of working patterns and potential correlations between attendance behavior and attrition, offering a fine-grained temporal perspective on employee engagement.\n",
    "Together, these anonymized datasets provide a multi-dimensional view of employees, combining objective HR data, managerial evaluations, subjective satisfaction measures, and actual attendance records in 2015, and the turnover outcome relates to departures in 2016. This rich data environment enables a thorough exploration of factors impacting turnover at HumanForYou.\n",
    "\n",
    "<br><br>\n",
    "### 2.2 Analysis According to ALTAI Ethical Requirements\n",
    "\n",
    "The approach adopted for this project is grounded in the principles of trustworthy artificial intelligence, as outlined by the European Commission’s Assessment List for Trustworthy Artificial Intelligence (ALTAI). These principles include respect for human autonomy, technical robustness and security, data privacy and governance, transparency, fairness and non-discrimination, societal well-being, and accountability. \n",
    "\n",
    "<center><img src=\"assets/ethic.png\" alt=\"map\" width=\"400\" height=\"400\"></center>\n",
    "\n",
    "<br><br>\n",
    "### 2.3 Analysis According to ALTAI Ethical Requirements\n",
    "\n",
    "The tool supports the actionability the key requirements outlined by the Ethics Guidelines for Trustworthy Artificial Intelligence (AI), presented by  the High-Level Expert Group on AI (AI HLEG) presented to the European Commission, in April 2019. The Ethics Guidelines introduced the concept of Trustworthy AI, based on seven key requirements:\n",
    "\n",
    "-\tHuman Autonomy and Oversight\n",
    "\n",
    "-\tTechnical Robustness and Safety\n",
    "\n",
    "-\tData Privacy and Governance\n",
    "\n",
    "-\tTransparency\n",
    "\n",
    "-\tDiversity, Non-Discrimination and Fairness\n",
    "\n",
    "-\tSocietal and Environmental Well-being\n",
    "\n",
    "-\tAccountability\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3e11c31c1548a6a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data loading and preparation\n",
    "\n",
    "### 3.1 Data import"
   ],
   "id": "5e09162196c1799f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "id": "7716752675b87495"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Loading data\n",
    "\n",
    "Here we display the first 5 rows of each file:"
   ],
   "id": "ac277fb05a87f7d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load dataset\n",
    "general_data = pd.read_csv('general_data.csv')\n",
    "\n",
    "# Display the first lines of the dataset\n",
    "general_data.head()"
   ],
   "id": "d713758314ca498c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "employee_survey_data = pd.read_csv('employee_survey_data.csv')\n",
    "employee_survey_data.head()\n"
   ],
   "id": "9d6ee4b7929248d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "manager_survey_data = pd.read_csv('manager_survey_data.csv')\n",
    "manager_survey_data.head()"
   ],
   "id": "615844a29ff20ea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "intime = pd.read_csv('in_time.csv')\n",
    "intime.head()"
   ],
   "id": "c8d83160b71208fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outtime = pd.read_csv('out_time.csv')\n",
    "outtime.head()"
   ],
   "id": "648874827d7e2788"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge Datasets",
   "id": "d88b28477edccf01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge datasets\n",
    "data = pd.merge(general_data, manager_survey_data, on='EmployeeID')\n",
    "data = pd.merge(data, employee_survey_data, on='EmployeeID')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"  Merged data: {len(data)} employees\")\n",
    "print(f\"   Columns: {len(data.columns)}\")\n",
    "print(\"First 5 columns:\", list(data.columns[:5]))"
   ],
   "id": "9da8348248423231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "### 1. handling missing values"
   ],
   "id": "f40929b91ef59bf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 1. Sélection STRICTE des colonnes numériques\n",
    "general_num = general_data.select_dtypes(include=\"number\")\n",
    "\n",
    "# 2. Comptage des valeurs manquantes (NaN) uniquement sur les colonnes numériques\n",
    "missing_per_column = general_num.isna().sum()\n",
    "\n",
    "print(\"Valeurs manquantes par colonne numérique :\")\n",
    "print(missing_per_column)\n",
    "\n",
    "# 3. Nombre total de valeurs manquantes numériques\n",
    "total_missing = missing_per_column.sum()\n",
    "print(f\"\\nNombre total de valeurs manquantes (numériques) : {total_missing}\")\n",
    "\n",
    "# 4. Imputation par la médiane\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "general_num_imputed_array = imputer.fit_transform(general_num)\n",
    "\n",
    "# 5. Reconstruction du DataFrame en conservant index et colonnes\n",
    "general_num_imputed = pd.DataFrame(\n",
    "    general_num_imputed_array,\n",
    "    columns=general_num.columns,\n",
    "    index=general_data.index\n",
    ")\n",
    "\n",
    "# 6. Vérification post-imputation\n",
    "print(\"\\nValeurs manquantes après imputation :\")\n",
    "print(general_num_imputed.isna().sum())"
   ],
   "id": "2f62eb317cb1981c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Sélection STRICTE des colonnes numériques\n",
    "employee_num = employee_survey_data.select_dtypes(include=\"number\")\n",
    "\n",
    "# 2. Comptage des valeurs manquantes (NaN) uniquement sur les colonnes numériques\n",
    "missing_per_column = employee_num.isna().sum()\n",
    "\n",
    "print(\"Valeurs manquantes par colonne numérique :\")\n",
    "print(missing_per_column)\n",
    "\n",
    "# 3. Nombre total de valeurs manquantes numériques\n",
    "total_missing = missing_per_column.sum()\n",
    "print(f\"\\nNombre total de valeurs manquantes (numériques) : {total_missing}\")\n",
    "\n",
    "# 4. Imputation par la médiane\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "employee_num_imputed_array = imputer.fit_transform(employee_num)\n",
    "\n",
    "# 5. Reconstruction du DataFrame en conservant index et colonnes\n",
    "employee_num_imputed = pd.DataFrame(\n",
    "    employee_num_imputed_array,\n",
    "    columns=employee_num.columns,\n",
    "    index=employee_survey_data.index\n",
    ")\n",
    "\n",
    "# 6. Vérification post-imputation\n",
    "print(\"\\nValeurs manquantes après imputation :\")\n",
    "print(employee_num_imputed.isna().sum())\n"
   ],
   "id": "f5c089e543303412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Sélection STRICTE des colonnes numériques\n",
    "manager_num = manager_survey_data.select_dtypes(include=\"number\")\n",
    "\n",
    "# 2. Comptage des valeurs manquantes (NaN) uniquement sur les colonnes numériques\n",
    "missing_per_column = manager_num.isna().sum()\n",
    "\n",
    "print(\"Valeurs manquantes par colonne numérique :\")\n",
    "print(missing_per_column)\n",
    "\n",
    "# 3. Nombre total de valeurs manquantes numériques\n",
    "total_missing = missing_per_column.sum()\n",
    "print(f\"\\nNombre total de valeurs manquantes (numériques) : {total_missing}\")\n",
    "\n",
    "# 4. Imputation par la médiane\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "manager_num_imputed_array = imputer.fit_transform(manager_num)\n",
    "\n",
    "# 5. Reconstruction du DataFrame en conservant index et colonnes\n",
    "manager_num_imputed = pd.DataFrame(\n",
    "    manager_num_imputed_array,\n",
    "    columns=manager_num.columns,\n",
    "    index=manager_survey_data.index\n",
    ")\n",
    "\n",
    "# 6. Vérification post-imputation\n",
    "print(\"\\nValeurs manquantes après imputation :\")\n",
    "print(manager_num_imputed.isna().sum())\n"
   ],
   "id": "78844c133d3d51bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove columns with more than 50% missing values\n",
    "threshold = 0.5\n",
    "intime_cleaned = intime.loc[:, intime.isnull().mean() < threshold]\n",
    "\n",
    "print(f\"Columns before: {intime.shape[1]}\")\n",
    "print(f\"Columns after: {intime_cleaned.shape[1]}\")\n",
    "\n",
    "# Display missing values for remaining columns\n",
    "print(\"\\nMissing values in remaining columns:\")\n",
    "print(intime_cleaned.isnull().sum())"
   ],
   "id": "1647674684bf235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove columns with more than 50% missing values\n",
    "threshold = 0.5\n",
    "outtime_cleaned = outtime.loc[:, outtime.isnull().mean() < threshold]\n",
    "\n",
    "print(f\"Columns before: {outtime.shape[1]}\")\n",
    "print(f\"Columns after: {outtime_cleaned.shape[1]}\")\n",
    "\n",
    "# Display missing values for remaining columns\n",
    "print(\"\\nMissing values in remaining columns:\")\n",
    "print(outtime_cleaned.isnull().sum())"
   ],
   "id": "e85b1d3055cc0fc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Encoding categorical variables",
   "id": "f6e4a10756017ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Select category columns\n",
    "general_cat = general_data.select_dtypes(include=[object])\n",
    "\n",
    "# 2. Encoder (dense array)\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)  # ou sparse=False si version plus ancienne\n",
    "\n",
    "# 3. Fit and Transform\n",
    "general_cat_encoded_array = cat_encoder.fit_transform(general_cat)\n",
    "\n",
    "# 4. Convertir en entiers (0/1 au lieu de 0.0/1.0)\n",
    "general_cat_encoded_array = general_cat_encoded_array.astype(int)\n",
    "\n",
    "# 5. Get the Feature Names\n",
    "feature_names = cat_encoder.get_feature_names_out(general_cat.columns)\n",
    "\n",
    "# 6. DataFrame final\n",
    "general_cat_encoded = pd.DataFrame(general_cat_encoded_array, columns=feature_names)\n",
    "\n",
    "general_cat_encoded.head()\n",
    "\n",
    "# Créer la nouvelle colonne Attrition\n",
    "general_cat_encoded[\"Attrition\"] = general_cat_encoded[\"Attrition_Yes\"]\n",
    "general_cat_encoded[\"Gender\"] = general_cat_encoded[\"Gender_Female\"]\n",
    "\n",
    "# Optionnel : supprimer les colonnes one-hot d'origine\n",
    "general_cat_encoded.drop(\n",
    "    columns=[\"Attrition_No\", \"Attrition_Yes\", \"Over18_Y\", \"Gender_Female\",\"Gender_Male\" ], #on vire Over18 car déductible depuis la variable age\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "general_cat_encoded.head()"
   ],
   "id": "2aa3690e3cdfdd22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Data normalization\n",
    "\n",
    "<center><img src=\"scale.png\" alt=\"map\" width=\"600\" height=\"200\"></center>"
   ],
   "id": "1a0f0e365915e458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardizing numeric columns\n",
    "scaler = StandardScaler()\n",
    "general_num_scaled = pd.DataFrame(scaler.fit_transform(general_num_imputed), columns=general_num_imputed.columns)\n",
    "\n",
    "# Verification of standardization\n",
    "general_num_scaled.head()\n"
   ],
   "id": "34ddb6f29af749e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"assets/standardization.png\" alt=\"map\" width=\"650\" height=\"250\"></center>\n",
    "<br><br><br>"
   ],
   "id": "3b17f0bea2b94ffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\n",
    "employee_num_scaled = pd.DataFrame(scaler.fit_transform(employee_num_imputed), columns=employee_num_imputed.columns)\n",
    "\n",
    "employee_num_scaled.head()"
   ],
   "id": "d242d9aa63c2dc39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\n",
    "manager_num_scaled = pd.DataFrame(scaler.fit_transform(manager_num_imputed), columns=manager_num_imputed.columns)\n",
    "\n",
    "manager_num_scaled.head()"
   ],
   "id": "2162d929e63468a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At this point, we loaded and prepared our datasets for analysis. We have imputed missing values, encoded categorical variables and normalized numerical variables. The data is now ready for the modeling and classification phase.",
   "id": "9e18d5b72204cbc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration and Visualization\n",
    "\n",
    "### 1. Analyzing descriptive statistics\n",
    "\n",
    "Before diving into data visualization, let's take a look at descriptive statistics to get an initial idea of variable distributions."
   ],
   "id": "ba7b738fe704ead0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "general_data.describe()\n",
   "id": "eada5069fb1a3a9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "employee_survey_data.describe()\n",
   "id": "e7a0d24a86480276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "manager_survey_data.describe()\n",
   "id": "a38d338649b5c2fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the dataset is complete for most variables (4,410 observations) but that the temporal variables (intime/outtime) are unusable. Outliers found, for example, in the MonthlyIncome column",
   "id": "205f648bfa0cd375"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Visualizing Numerical Variables\n",
    "We'll start by visualizing the distributions of numerical variables to better understand their behavior."
   ],
   "id": "1d10cc1e39f92d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "general_num_scaled.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n"
   ],
   "id": "501141cd7592e371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "employee_num_scaled.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "id": "9f4d384872aee61f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "manager_num_scaled.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "id": "260ccf13cbf3b72d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Correlation matrix\n",
    "To explore the relationships between numerical variables, we use a correlation matrix and a heatmap."
   ],
   "id": "86910a58a3c7ac10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "non_zero_std_cols = general_num_scaled.columns[general_num_scaled.std() != 0]\n",
    "general_num_scaled_clean = general_num_scaled[non_zero_std_cols]\n",
    "\n",
    "employee_num_scaled.drop(\n",
    "    columns=[\"EmployeeID\" ],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "manager_num_scaled.drop(\n",
    "    columns=[\"EmployeeID\" ],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# 2. Concatenate numerical and categorical features\n",
    "X = pd.concat(\n",
    "    [general_num_scaled_clean, general_cat_encoded,employee_num_scaled, manager_num_scaled],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3. Correlation matrix (NUMERICAL FEATURES ONLY)\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "# 4. Plot correlation heatmap\n",
    "plt.figure(figsize=(40, 16))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    square=True\n",
    ")\n",
    "plt.title('Matrice de Corrélation des Variables Numériques')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a637f1a321a7e867"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Visualizing relationships between variables\n",
    "\n",
    "We're going to create a few scatter plots to visualize the relationships between certain key variables."
   ],
   "id": "7a90a0611618de6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"Age\",\n",
    "    \"DistanceFromHome\", \n",
    "    \"Education\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"NumCompaniesWorked\",\n",
    "    \"PercentSalaryHike\",\n",
    "    \"TotalWorkingYears\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"YearsSinceLastPromotion\",\n",
    "    \"YearsWithCurrManager\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    general_data[attributes], \n",
    "    figsize=(15, 15),\n",
    "    alpha=0.5,\n",
    "    diagonal='hist' \n",
    ")\n",
    "plt.suptitle('Scatter Matrix - General Variables', y=1.0)\n",
    "plt.show()"
   ],
   "id": "8aaedd17d38caa2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Salary analysis**",
   "id": "7224296fa4e0dfbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"MonthlyIncome\",\n",
    "    \"TotalWorkingYears\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"Education\",\n",
    "    \"JobLevel\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    general_data[attributes], \n",
    "    figsize=(15, 15),\n",
    "    alpha=0.5,\n",
    "    diagonal='hist'  \n",
    ")\n",
    "plt.suptitle('Scatter Matrix - Salary Analysis', y=1.0)\n",
    "plt.show()"
   ],
   "id": "e02f17e148f9e0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Seniority Analysis**",
   "id": "f0936d29b7942050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"Age\",\n",
    "    \"TotalWorkingYears\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"YearsWithCurrManager\",\n",
    "    \"NumCompaniesWorked\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    general_data[attributes], \n",
    "    figsize=(15, 15),\n",
    "    alpha=0.5,\n",
    "    diagonal='hist'  \n",
    ")\n",
    "plt.suptitle('Scatter Matrix -  Seniority Analysis', y=1.0)\n",
    "plt.show()"
   ],
   "id": "e37060a42c8795c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Mobility Analysis**",
   "id": "1cd5e905ceb1b96f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"NumCompaniesWorked\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"Age\",\n",
    "    \"DistanceFromHome\",\n",
    "    \"JobLevel\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    general_data[attributes], \n",
    "    figsize=(15, 15),\n",
    "    alpha=0.5,\n",
    "    diagonal='hist'  \n",
    ")\n",
    "plt.suptitle('Scatter Matrix -  Mobility Analysis', y=1.0)\n",
    "plt.show()"
   ],
   "id": "8066fa47ef2f2bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Satisfaction Analysis**",
   "id": "c022df999fc6eb2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"EnvironmentSatisfaction\",\n",
    "    \"JobSatisfaction\",\n",
    "    \"WorkLifeBalance\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    employee_survey_data[attributes], \n",
    "    figsize=(10, 10),\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.suptitle('Scatter Matrix -  Satisfaction Analysis', y=1.0)\n",
    "plt.show()\n"
   ],
   "id": "370b84994725e0f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Performance Analysis**",
   "id": "87a6fc783aca4afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attributes = [\n",
    "    \"JobInvolvement\",\n",
    "    \"PerformanceRating\"\n",
    "]\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(\n",
    "    manager_survey_data[attributes], \n",
    "    figsize=(8, 8),\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.suptitle('Scatter Matrix -  Performance Analysis', y=1.0)\n",
    "plt.show()"
   ],
   "id": "ee599418019fba4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Do not include:\n",
    "<br>\n",
    "“EmployeeID” = Meaningless identifier\n",
    "<br>\n",
    "“EmployeeCount” = Constant (always 1)\n",
    "<br>\n",
    "“StandardHours” = Constant (always 8)\n",
    "<br>\n",
    "“StockOptionLevel” = Little variance, zero correlations"
   ],
   "id": "d103ca9c7da06154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ethics\n",
    "### 1. Global ethical methodology\n",
    "\n",
    "The approach adopted for this project is grounded in the principles of trustworthy artificial intelligence, as outlined by the European Commission’s Assessment List for Trustworthy Artificial Intelligence (ALTAI). These principles include respect for human autonomy, technical robustness and security, data privacy and governance, transparency, fairness and non-discrimination, societal well-being, and accountability. \n",
    "\n",
    "<br>\n",
    "\n",
    "### 2. Analysis According to ALTAI Ethical Requirements\n",
    "\n",
    "The tool supports the actionability the key requirements outlined by the Assessment List for Trustworthy Artificial Intelligence, presented by the High-Level Expert Group on AIpresented to the European Commission, in April 2019. Despite our company not being in the EU, this document is an excellent base to build upon, given no Indian equivalent exists. The Ethics Guidelines introduces the concept of Trustworthy AI, based on seven key requirements:\n",
    "\n",
    "\n",
    "- Human Autonomy and Oversight: AI systems should support human agency and human decision-making, as prescribed by the principle of respect for human autonomy. This includes training individuals to make informed choices, claim their fundamental rights, and ensuring appropriate oversight mechanisms.\n",
    "\n",
    "- Technical Robustness and Safety: The AI systems must deliver systems that can be trusted and that are resilient. This implies the systems must be implemented with a preventative approach to limit potential unwanted harm and prevent it whenever it could happen.\n",
    "\n",
    "- Data Privacy and Governance: To ensure trustworthy AI, robust protections for privacy and data quality must be in place throughout the system's lifecycle. This includes respecting data protection regulations (such as DPDP 2023, the Digital Personal Data Protection Act in India), implementing strong mechanisms for data collection, storage, and processing, minimizing data usage where possible, and ensuring high-quality, accurate, and relevant datasets to avoid errors or biases emergingfrom poor data management.\n",
    "\n",
    "- Transparency: Everything should be traceable, explainable and there should be clear communications about the limitations of the system. This aims to build trust towards AI and to have a good comprehension of its whole way of working.\n",
    "\n",
    "- Diversity, Non-Discrimination and Fairness: To achieve Trustworthy AI, we must have inclusion and diversity throughout the entire AI system. AI systems may suffer from the inclusion of inadvertent historic bias, incompleteness, and bad governance models. If handled poorly, those biases could lead to unintended direct or indirect prejudice against a certain group of people. We aim to avoid potential harm at any cost.\n",
    "\n",
    "- Societal and Environmental Well-being: Trustworthy AI should contribute positively to society and the environment, promoting sustainable development and minimizing negative impacts. This involves assessing and addressing potential effects on employment, social cohesion, democracy, and ecological footprint.\n",
    "\n",
    "- Accountability: Mechanisms must be established to ensure responsibility and accountability for AI systems and their outcomes, both before and after deployment. This includes clear allocation of responsibilities among developers, deployers, and users, auditability of processes and decisions, redress mechanisms for affected individuals, and regular risk assessments to enable traceability and remediation in case of harm.\n",
    "\n",
    "<center><img src=\"ethic.png\" alt=\"map\" width=\"400\" height=\"400\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. Removed features\n",
    "\n",
    "According to the ALTAI, some data must be avoided or excluded prior to using them to train our model. This includes the following data, present in our dataset:\n",
    "\n",
    "- Gender: This is potentially sensitive data if it reveals or infers sexual orientation. More broadly, it is a protected characteristic against discrimination. Using it can introduce discriminatory biases, violating ALTAI's non-discrimination principle\n",
    "\n",
    "- Marital status: This data reveals aspects of private and family life. It can indirectly infer sensitive information.\n",
    "\n",
    "- Education field: Depending on the field, it can reveal religious or philosophical beliefs or ethnic origin.\n",
    "\n",
    "- Age: Using it in a predictive model can lead to ageist biases. ALTAI recommends minimising proxies for protected characteristics to avoid discrimination.\n",
    "\n",
    "- Employee ID: Allows to directly identify an employee.\n",
    "\n",
    "\n",
    "For efficiency in the handling of data, we also delete some useless fields of the data:\n",
    "\n",
    "- Over18 is non-variable (always yes)\n",
    "- Standard hours is always at 8 hours a day\n",
    "- Employee count is always at 1."
   ],
   "id": "58750091de14f07f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification models\n",
    "In this section, we will develop and evaluate multiple classification models to identify the best performer for our objectives. We will implement various algorithms, assess their performance using standard metrics, and compare results to select the optimal model."
   ],
   "id": "7629bf1438c631b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Reminder of the main performance metrics\n",
    "\n",
    "Performance metrics are essential for assessing the effectiveness of classification models. Here's a reminder of the main metrics used:\n",
    "\n",
    "#### 1. Accuracy\n",
    "Accuracy is the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$\n",
    "\n",
    "#### 2. Precision\n",
    "Precision is the ratio of true positives (TP) to the sum of true positives (TP) and false positives (FP).\n",
    "\n",
    "$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$\n",
    "\n",
    "#### 3. Recall\n",
    "Recall is the ratio of true positives (TP) to the sum of true positives (TP) and false negatives (FN).\n",
    "\n",
    "$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$\n",
    "\n",
    "#### 4. F1-Score\n",
    "The F1-score is the harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a method of visualizing the performance of a classification model. It displays results in tabular form, with actual and predicted predictions. Its components are :\n",
    "\n",
    "- **True Positives (TP)**: Number of times the positive class was correctly predicted.\n",
    "- **False Positive (FP)**: Number of times the negative class was incorrectly predicted as positive.\n",
    "- **True Negatives (TN)**: Number of times the negative class was correctly predicted.\n",
    "- **False Negatives (FN)**: Number of times the positive class was incorrectly predicted as negative.\n",
    "\n",
    "$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& \\text{Predicted Positive} & \\text{Predicted Negative} \\\\\n",
    "\\hline\n",
    "\\text{True Positive} & \\text{TP} & \\text{FN} \\\\\n",
    "\\hline\n",
    "\\text{True Negative} & \\text{FP} & \\text{TN} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "<center><img src=\"assets/metrics.png\" alt=\"map\" width=\"550\" height=\"400\"></center>\n",
    "<br><br><br>\n",
    "\n",
    "### AUC and ROC curve\n",
    "\n",
    "#### ROC curve (Receiver Operating Characteristic)\n",
    "\n",
    "The ROC curve is a graph showing the performance of a classification model at different discrimination thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR).\n",
    "\n",
    "- True Positive Rate (TPR)** : This is the recall.\n",
    "\n",
    "$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$\n",
    "\n",
    "- False Positive Rate (FPR)** : This is the ratio of false positives to the sum of true negatives and false positives.\n",
    "\n",
    "$\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "$\n",
    "\n",
    "#### AUC (Area Under the Curve)\n",
    "\n",
    "AUC is the area under the ROC curve. It measures the model's ability to distinguish between positive and negative classes. An AUC of 1.0 indicates a perfect model, while an AUC of 0.5 indicates a model that does no better than random choice.\n",
    "\n",
    "- **AUC interpretation** :\n",
    "  - **0.9 - 1** : Excellent performance\n",
    "  - **0.8 - 0.9** : Good performance\n",
    "  - **0.7 - 0.8** : Acceptable performance\n",
    "  - **0.6 - 0.7** : Poor performance\n",
    "  - **0.5 - 0.6** : Very poor performance\n",
    "\n",
    "ROC curves and AUC scores are valuable tools for comparing the performance of different classification models, particularly in situations where classes are unbalanced.\n",
    "\n",
    "By using these metrics and tools, you can comprehensively assess the performance of your classification models and choose the one best suited to your problem."
   ],
   "id": "e862f27031ff1ba4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Target variable and class balance\n",
    "\n",
    "Before training any model, we analyze the target variable `Attrition`.\n",
    "This step is critical because class imbalance strongly impacts:\n",
    "- model behavior (e.g., a model may simply learn to always predict the majority class),\n",
    "- the interpretation of performance metrics (e.g., accuracy can be misleading when one class dominates),\n",
    "- the business conclusions we draw from the results (e.g., underestimating the number of employees at risk).\n",
    "\n",
    "In our dataset, `Attrition` is a binary variable indicating whether an employee has left the organization (`Yes`) or not (`No`).\n",
    "Understanding the proportion of each class (churned vs. retained employees) will guide the choice of appropriate evaluation metrics and, if necessary, techniques to handle imbalance (such as resampling or class weights)."
   ],
   "id": "f2115ab66f49fcd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model application",
   "id": "d1d84c65b19b4c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Preparation of learning and test data\n",
   "id": "3eb6d443aa7598c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Feature matrix (X) and target vector (y)\n",
   "id": "74648e7b19119f45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now construct the feature matrix `X` and the target vector `y` that will be used as inputs for the classification models.\n",
    "\n",
    "- The **target variable** `y` corresponds to the column `Attrition`, which we convert into a binary numerical format:\n",
    "  - `No` → `0` (the employee stayed),\n",
    "  - `Yes` → `1` (the employee left).\n",
    "- The **feature matrix** `X` contains all the explanatory variables used to predict attrition.\n",
    "  We remove:\n",
    "  - the target column `Attrition`, to avoid data leakage,\n",
    "  - the column `EmployeeID`, which is an identifier and does not carry predictive information.\n",
    "\n",
    "The corresponding code is:"
   ],
   "id": "2e3e56ebc5c7cdd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = data.drop(columns=[\"Attrition\", \"EmployeeID\"])\n",
    "y = data[\"Attrition\"].astype(str).str.strip().map({\"No\": 0, \"Yes\": 1}).astype(int)\n",
    "X.shape, y.shape\n"
   ],
   "id": "d2f8ac7512853a61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Train / Test split\n",
    "\n",
    "- We split the dataset into training and test sets.\n",
    "Stratification is used to preserve class imbalance proportions.\n",
    "- We now split the dataset into a **training set** and a **test set** in order to evaluate our models on unseen data.\n",
    "\n",
    "- The **training set** is used to fit (learn) the model parameters.\n",
    "- The **test set** is kept aside and only used at the end to assess the generalization performance.\n",
    "\n",
    "We use a **70/30 split** (`test_size=0.30`), meaning 70% of the data for training and 30% for testing.\n",
    "\n",
    "To maintain the original class distribution of `Attrition` in both subsets, we enable **stratification** with `stratify=y`.\n",
    "This is particularly important when the target variable is imbalanced, as it ensures that both the training and test sets reflect the same proportion of positive and negative cases.\n"
   ],
   "id": "c5638db4b9f74c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ],
   "id": "d1c64e82e3496248"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Show imbalance (counts + % + plot)\n",
    "- We first inspect the distribution of the target variable `Attrition` to quantify the class imbalance.\n",
    "\n",
    "- **Counts** tell us how many employees belong to each class (`No` vs. `Yes`).\n",
    "- **Percentages** indicate the relative frequency of each class in the dataset.\n",
    "- A **bar plot** visually highlights how skewed the classes are.\n",
    "\n",
    "The following code computes and displays these elements:"
   ],
   "id": "b94bc7f705a6bd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = y.value_counts().sort_index()\n",
    "pct = (counts / counts.sum() * 100).round(2)\n",
    "\n",
    "print(\"Class counts (Attrition):\")\n",
    "print(counts)\n",
    "print(\"\\nClass percentages (%):\")\n",
    "print(pct)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([\"0 (Stay)\", \"1 (Leave)\"], [counts.get(0,0), counts.get(1,0)])\n",
    "plt.title(\"Attrition class distribution\")\n",
    "plt.ylabel(\"Number of employees\")\n",
    "plt.show()\n"
   ],
   "id": "f94f5524ca3a0cda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.Build preprocessing (works with mixed types)\n",
    "\n",
    "Our feature matrix `X` contains both **numerical** and **categorical** variables.\n",
    "To feed these into most machine learning models, we create a preprocessing pipeline that:\n",
    "\n",
    "- **Separates columns by type**:\n",
    "  - `num_cols`: numerical features,\n",
    "  - `cat_cols`: categorical features.\n",
    "- **Applies appropriate transformations**:\n",
    "  - For **numerical features**:\n",
    "    - Impute missing values with the **median**,\n",
    "    - Standardize features using `StandardScaler` (zero mean, unit variance).\n",
    "  - For **categorical features**:\n",
    "    - Impute missing values with the **most frequent** category,\n",
    "    - Apply **one-hot encoding** to convert categories into binary indicator variables.\n",
    "\n",
    "We use a `ColumnTransformer` to apply these pipelines to the correct subsets of columns, and drop any remaining columns."
   ],
   "id": "bea233eba206e860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "preprocess\n"
   ],
   "id": "30fe8618b2ae9ece"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Logistic regression",
   "id": "7a00e1cc0778aa75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Logistic regression reminder\n",
    "\n",
    "Logistic regression is a statistical technique used to model the probability of a binary event (with two possible outcomes) occurring. Unlike linear regression, which predicts a continuous value, logistic regression predicts the probability of an event occurring.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "Logistic regression uses the logistic or sigmoid function to transform the output of linear regression into a probability.\n",
    "\n",
    "The logistic function is defined as follows:\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "In logistic regression, \\( z \\) is a linear combination of the features:\n",
    "$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$\n",
    "\n",
    "Where:\n",
    "- $\\beta_0$ is the intercept (y-intercept)\n",
    "- $\\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients of the characteristics $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "The probability of the event occurring (for example, \\( y = 1 \\)) is then given by :\n",
    "$P(y=1|x) = \\sigma(z) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)}}$\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "The cost function used to adjust the logistic regression parameters is the log-likelihood, defined as follows:\n",
    "$J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] $\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of samples\n",
    "- $y^{(i)}$ is the actual value for sample $i$.\n",
    "- $\\hat{y}^{(i)}$ is the predicted probability for sample $i$.\n",
    "\n",
    "#### Model training\n",
    "\n",
    "Training the logistic regression model involves finding the $\\beta$ parameters that minimize the cost function. This is generally done using the gradient descent algorithm.\n"
   ],
   "id": "17ffe637ebdf1a6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n"
   ],
   "id": "4498ae396782fd93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Perceptron",
   "id": "2a1dae00783ddc2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A reminder of the Perceptron\n",
    "\n",
    "The perceptron is one of the simplest and oldest supervised classification algorithms, introduced by Frank Rosenblatt in 1957. It is an elementary processing unit of a neural network, often used for binary classification tasks. The perceptron is based on a linear combination of input features and uses a threshold function to produce a binary output.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "The perceptron calculates a weighted sum of the input features and applies a threshold function to determine the predicted class.\n",
    "\n",
    "The perceptron output is defined as follows:\n",
    "$\\hat{y} = \\begin{cases}\n",
    "1 & \\text{si } z \\geq 0\n",
    "0 & \\text{si } z < 0\n",
    "\\end{cases} $\n",
    "\n",
    "Where:\n",
    "$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$\n",
    "\n",
    "Here, $\\beta_0$ is the bias (or intercept), and $\\beta_1, \\beta_2, \\ldots, \\beta_n$ are the feature weights $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "#### Learning algorithm\n",
    "\n",
    "The perceptron's learning algorithm adjusts weights according to classification errors. For each training sample $(x^{(i)}, y^{(i)})$, where $y^{(i)}$ is the actual class:\n",
    "\n",
    "1. Calculate the predicted output:\n",
    "$hat{y}^{(i)} = \\begin{cases}\n",
    "1 & \\text{si } z \\geq 0\n",
    "0 & \\text{si } z < 0\n",
    "\\end{cases}$\n",
    "\n",
    "2. Update the weights if the prediction is incorrect:\n",
    "$\\beta_j = \\beta_j + \\eta (y^{(i)} - \\hat{y}^{(i)}) x_j^{(i)}$\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "The perceptron does not use a cost function in the traditional sense such as logistic regression. Weights are updated directly according to classification errors.\n"
   ],
   "id": "67abda3650b13cff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", Perceptron(random_state=42))\n",
    "])\n",
    "perceptron.fit(X_train, y_train)"
   ],
   "id": "ba5116ffb1aa3c32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Question:** According to the results obtained, how does Perceptron differ from logistic regression in its approach to classification?",
   "id": "ba667e5c08b07834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Random Forest",
   "id": "382bdfe820205690"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reminder of Random Forests\n",
    "\n",
    "Random Forests are a powerful and flexible ensemble method used for classification and regression tasks. They combine several decision trees to improve predictive performance and reduce the risk of overlearning.\n",
    "\n",
    "#### How it works\n",
    "\n",
    "A random forest is made up of many independent decision trees, each built on a random sample of the training data and using a random subset of the features for each division of the tree. The predictions of all the trees are then combined to produce a single final prediction.\n",
    "\n",
    "#### Random Forest construction\n",
    "\n",
    "1. **Bootstrap sampling**: For each tree in the forest, a random sample with bootstrap replacement of the training data is created. This means that some examples may be selected several times, while others may not be selected at all.\n",
    "2. **Feature Subset Selection**: At each node of each tree, a random subset of the features is selected. The tree selects the best division from this subset of features.\n",
    "3. **Tree construction**: Decision trees are built to completion without pruning. This allows each tree to capture complex patterns in the data.\n",
    "4. **Aggregation of Predictions**: For classification, each tree votes for a class, and the class with the most votes is chosen as the final prediction (majority voting). For regression, the average of all tree predictions is used.\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "##### Advantages :\n",
    "- **Reduces overlearning**: By combining the predictions of several trees, random forests reduce the risk of overlearning compared with individual decision trees.\n",
    "- **Robustness**: Insensitive to variations in training data. Random forests are less sensitive to fluctuations in training data.\n",
    "- **Feature Management**: Able to manage a large number of features and determine the most important ones.\n",
    "- **Missing Data Handling**: Can handle missing values by imputing values based on forest trees.\n",
    "\n",
    "##### Disadvantages :\n",
    "- **Complexity and Computing Time** : Random forests require more computation time and memory than individual decision trees, especially when the number of trees is high.\n",
    "- **Interpretability**: Less interpretable than individual decision trees, due to the combination of many trees.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "- **Classification**: Used for classification tasks in diverse fields such as finance, medicine and marketing.\n",
    "- **Regression**: Prediction of continuous values in contexts such as real estate price forecasting and sales prediction.\n",
    "- **Feature Selection**: Identification of the most important features for prediction.\n",
    "\n",
    "Random forests are a powerful tool for improving the predictive performance and robustness of decision models, by combining the strength of multiple decision trees while mitigating their individual weaknesses.\n"
   ],
   "id": "41c8c4d63d9d8063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n"
   ],
   "id": "8b623ff159ca1635"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparative study of SK-Learn models",
   "id": "bfa796cb362129f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Preparation of learning and test data\n",
    "We split the dataset into training and test sets using a 70/30 proportion.Stratification on the target ` y `is used to preserve the original class imbalance (same proportion of attrition/no attrition in both sets )\n"
   ],
   "id": "1e792e2ae0a6af19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y must already exist:\n",
    "# X = data.drop(columns=[\"Attrition\",\"EmployeeID\"])\n",
    "# y = data[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ],
   "id": "8c192cb045aa9262"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Show imbalance (counts + % + plot)\n",
    "\n",
    "- We examine the distribution of the target variable `Attrition` (`0 = Stay`, `1 = Leave`)\n",
    "using class counts, class percentages, and a bar plot."
   ],
   "id": "3b739b81d179aae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "counts = y.value_counts().sort_index()\n",
    "pct = (counts / counts.sum() * 100).round(2)\n",
    "\n",
    "print(counts)\n",
    "print(pct)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([\"0 (Stay)\", \"1 (Leave)\"], [counts.loc[0], counts.loc[1]])\n",
    "plt.title(\"Attrition class distribution\")\n",
    "plt.ylabel(\"Number of employees\")\n",
    "plt.show()\n"
   ],
   "id": "ca32a73ec9441e26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.Build preprocessing (works with mixed types)\n",
    "- We build a preprocessing pipeline that handles numerical and categorical features differently:\n",
    "- numerical: median imputation + standardization\n",
    "- categorical: most frequent imputation + one‑hot encoding\n",
    "\n",
    "This pipeline will be combined later with our machine learning models."
   ],
   "id": "c25b6128dd02da47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "preprocess\n"
   ],
   "id": "6c5504d7b67ad454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.Define models as full pipelines (preprocess + model)",
   "id": "fa67fe964248b427"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, random_state=42, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"Perceptron\": Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", Perceptron(random_state=42))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "}\n"
   ],
   "id": "3e8947bb5728247"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Train models and compute predictions/scores",
   "id": "c94b85d51a23b27b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_score(pipeline, X_):\n",
    "    \"\"\"\n",
    "    Returns continuous scores for ROC/PR:\n",
    "    - predict_proba[:,1] if available\n",
    "    - else decision_function if available\n",
    "    - else None\n",
    "    \"\"\"\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        return pipeline.predict_proba(X_)[:, 1]\n",
    "    if hasattr(pipeline, \"decision_function\"):\n",
    "        return pipeline.decision_function(X_)\n",
    "    return None\n",
    "\n",
    "predictions = {}\n",
    "scores = {}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions[name] = pipe.predict(X_test)\n",
    "    scores[name] = get_score(pipe, X_test)\n",
    "\n",
    "list(predictions.keys()), {k: (None if v is None else \"ok\") for k,v in scores.items()}\n"
   ],
   "id": "c6fbc793a47fc657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.Fit + predict + get continuous scores (ROC/PR)\n",
    "- We now fit each pipeline on the training data, obtain:\n",
    "- **hard predictions** (`predict`) for confusion matrices and basic metrics,\n",
    "- **continuous scores** (probabilities or decision function) for ROC and PR curves."
   ],
   "id": "44b278a00d2f2ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_score(pipeline, X_):\n",
    "    \"\"\"\n",
    "    Returns continuous scores for ROC/PR:\n",
    "    - predict_proba[:,1] if available\n",
    "    - else decision_function if available\n",
    "    - else None\n",
    "    \"\"\"\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        return pipeline.predict_proba(X_)[:, 1]\n",
    "    if hasattr(pipeline, \"decision_function\"):\n",
    "        return pipeline.decision_function(X_)\n",
    "    return None\n",
    "\n",
    "predictions = {}\n",
    "scores = {}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions[name] = pipe.predict(X_test)\n",
    "    scores[name] = get_score(pipe, X_test)\n",
    "\n",
    "list(predictions.keys()), {k: (None if v is None else \"ok\") for k,v in scores.items()}\n"
   ],
   "id": "692f0020e3bbd8b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Compute evaluation metrics and build `results` dictionary\n",
    "\n",
    "We now compute, for each model:\n",
    "- the **confusion matrix**,\n",
    "- **Accuracy** and **Balanced Accuracy**,\n",
    "- **Precision**, **Recall**, and **F1-score** for the positive class (Attrition = 1),\n",
    "- **ROC AUC** and **PR AUC** (if continuous scores are available).\n",
    "\n",
    "All metrics are stored in a single `results` dictionary for easy comparison."
   ],
   "id": "e13fafd96602bca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 1) Build `results` (required)\n",
    "# ================================\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "def build_results(y_test, predictions, scores):\n",
    "    results = {}\n",
    "    for name in predictions.keys():\n",
    "        y_pred = predictions[name]\n",
    "        y_score = scores.get(name, None)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        rec  = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        f1v  = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "        if y_score is not None:\n",
    "            roc_auc = roc_auc_score(y_test, y_score)\n",
    "            pr_auc  = average_precision_score(y_test, y_score)\n",
    "\n",
    "        results[name] = {\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"Accuracy\": acc,\n",
    "            \"BalancedAcc\": bacc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1v,\n",
    "            \"ROC_AUC\": roc_auc,\n",
    "            \"PR_AUC\": pr_auc\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = build_results(y_test, predictions, scores)\n",
    "\n",
    "# ================================\n",
    "# 2) Plot confusion matrices\n",
    "# ================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = [0, 1]\n",
    "    plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "    plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for name, r in results.items():\n",
    "    plot_confusion_matrix(r[\"confusion_matrix\"], f\"Confusion Matrix — {name}\")\n"
   ],
   "id": "5e7bde384749e3f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Plot ROC and Precision–Recall curves\n",
    "\n",
    "Using the continuous scores (`scores`) computed earlier, we now:\n",
    "- plot the **ROC curve** (TPR vs FPR) for each model and display its **AUC**,\n",
    "- plot the **Precision–Recall curve** for each model and display its **Average Precision (AP)**."
   ],
   "id": "6b745ac41c28d0ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "for name in models.keys():\n",
    "    y_score = scores[name]\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    auc = roc_auc_score(y_test, y_score)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PR\n",
    "plt.figure(figsize=(6,5))\n",
    "for name in models.keys():\n",
    "    y_score = scores[name]\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_score)\n",
    "    ap = average_precision_score(y_test, y_score)\n",
    "    plt.plot(rec, prec, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "47b091f6ae394b3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Metrics table (model comparison)\n",
    "\n",
    "We build a single table to compare models on the same test set.\n",
    "For imbalanced attrition, prioritize:\n",
    "- Recall (class 1) and F1 (class 1)\n",
    "- Balanced Accuracy\n",
    "- PR-AUC (Average Precision)\n",
    "ROC-AUC is also reported when probability/decision scores are available.\n"
   ],
   "id": "f94453f4b3694ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, r in results.items():\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": r.get(\"Accuracy\", np.nan),\n",
    "        \"BalancedAccuracy\": r.get(\"BalancedAcc\", np.nan),\n",
    "        \"Precision(1)\": r.get(\"Precision\", np.nan),\n",
    "        \"Recall(1)\": r.get(\"Recall\", np.nan),\n",
    "        \"F1(1)\": r.get(\"F1\", np.nan),\n",
    "        \"ROC_AUC\": r.get(\"ROC_AUC\", np.nan),\n",
    "        \"PR_AUC\": r.get(\"PR_AUC\", np.nan)\n",
    "    })\n",
    "\n",
    "metrics_table = pd.DataFrame(rows)\n",
    "\n",
    "metrics_table\n"
   ],
   "id": "a434b4fda023b9fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Cross-Validation\n",
    "\n",
    "**Principle of Cross-Validation:**\n",
    "Cross-validation is a statistical method used to estimate the performance of a model on unseen data. It addresses the limitation of a single train-test split, which can be sensitive to how the data is partitioned.\n",
    "\n",
    "In **k-fold cross-validation**, the dataset is divided into *k* subsets (folds). The model is trained *k* times, each time using *k-1* folds for training and the remaining fold for validation. The final performance metric is the average of the results from all *k* trials. This provides a more robust and reliable estimate of the model\"s generalization ability."
   ],
   "id": "978b667b3ff2062b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"Evaluating models using 5-fold cross-validation (Accuracy):\")\n",
    "for name, model in models.items():\n",
    "    # Using the training set for cross-validation to assess stability\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    print(f\"{name:20}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ],
   "id": "a9522705f7abd4c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Feature Importance\n",
    "\n",
    "**Principle of Feature Importance:**\n",
    "Feature importance refers to techniques that assign scores to input features based on their contribution to the model\"s predictive performance. Understanding feature importance is crucial for:\n",
    "- **Model Interpretability:** Identifying which factors (e.g., salary, work-life balance, distance from home) most heavily influence model predictions.\n",
    "- **Feature Selection:** Identifying and potentially removing irrelevant or redundant features to simplify the model and improve performance.\n",
    "- **Business Insights:** Providing actionable insights to stakeholders about the primary drivers of the target outcome (employee attrition)."
   ],
   "id": "d71e9a9bcdf8cc77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We use the RandomForest model as it provides reliable native feature importance scores\n",
    "if \"RandomForest\" in models:\n",
    "    rf_pipeline = models[\"RandomForest\"]\n",
    "    # Ensure the model is fitted (it should be from previous steps)\n",
    "    rf_model = rf_pipeline.named_steps[\"model\"]\n",
    "    preprocessor = rf_pipeline.named_steps[\"preprocess\"]\n",
    "\n",
    "    # Get feature names from the preprocessor\n",
    "    # This requires scikit-learn >= 1.0\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    except:\n",
    "        # Fallback if get_feature_names_out is not available\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(rf_model.feature_importances_))]\n",
    "\n",
    "    # Get importance values\n",
    "    importances = rf_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for sorting and plotting\n",
    "    feat_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "    feat_df = feat_df.sort_values(by=\"Importance\", ascending=False).head(15)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_df, hue=\"Feature\", palette=\"magma\", legend=False)\n",
    "    plt.title(\"Top 15 Most Important Features - RandomForest\")\n",
    "    plt.xlabel(\"Relative Importance Score\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"RandomForest model not found in the models dictionary.\")\n",
    "## -----------------------------------------------------------------------------------"
   ],
   "id": "bd63d4995476a1f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ethical feature selection\n",
    "\n",
    "Based on ethical analysis and responsible AI principles, we remove features that may introduce\n",
    "discrimination, bias, or that have no causal relevance to employee attrition.\n",
    "\n",
    "### Removed features and justification\n",
    "\n",
    "- **Gender** → risk of gender discrimination\n",
    "- **MaritalStatus** → private life attribute, not job-related\n",
    "- **EducationField** → indirect socio-economic bias\n",
    "- **Age** → age discrimination risk\n",
    "- **EmployeeID** → identifier, no predictive value\n",
    "- **Over18** → constant value, no information\n",
    "- **StandardHours** → constant value\n",
    "- **EmployeeCount** → constant value\n",
    "\n",
    "These features are excluded before model training to ensure fairness, transparency,\n",
    "and compliance with ethical AI guidelines.\n"
   ],
   "id": "52dde2583b653ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# Ethical feature removal\n",
    "# ================================\n",
    "\n",
    "features_to_drop = [\n",
    "    \"Gender\",\n",
    "    \"MaritalStatus\",\n",
    "    \"EducationField\",\n",
    "    \"Age\",\n",
    "    \"EmployeeID\",\n",
    "    \"Over18\",\n",
    "    \"StandardHours\",\n",
    "    \"EmployeeCount\"\n",
    "]\n",
    "\n",
    "X_ethic = data.drop(columns=[\"Attrition\"] + features_to_drop)\n",
    "y_ethic = data[\"Attrition\"].astype(str).str.strip().map({\"No\": 0, \"Yes\": 1}).astype(int)\n",
    "\n",
    "X_ethic.shape, y_ethic.shape\n"
   ],
   "id": "e6b6b0d3a66f5cd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing after ethical filtering\n",
    "\n",
    "After removing sensitive and non-informative features, we rebuild the preprocessing pipeline:\n",
    "\n",
    "- Numerical features → median imputation + standardization\n",
    "- Categorical features → most frequent imputation + one-hot encoding\n",
    "\n",
    "This ensures consistency and prevents data leakage.\n"
   ],
   "id": "64bc91a2aa6cac46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cat_cols = X_ethic.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X_ethic.columns if c not in cat_cols]\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess_ethic = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols)\n",
    "])\n"
   ],
   "id": "f4a9127c8ae12431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model training with ethically filtered features\n",
    "\n",
    "We retrain the same models using the ethically filtered dataset:\n",
    "\n",
    "- Logistic Regression (baseline, interpretable)\n",
    "- Perceptron (linear classifier)\n",
    "- Random Forest (non-linear, high performance)\n",
    "\n",
    "This allows a direct comparison with previous results.\n"
   ],
   "id": "b9d006ed2250b747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ethic,\n",
    "    y_ethic,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y_ethic\n",
    ")\n"
   ],
   "id": "f959d8251f49336b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models_ethic = {\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        (\"preprocess\", preprocess_ethic),\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"Perceptron\": Pipeline([\n",
    "        (\"preprocess\", preprocess_ethic),\n",
    "        (\"model\", Perceptron(random_state=42))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"preprocess\", preprocess_ethic),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "}\n"
   ],
   "id": "500172c408246482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_score(pipeline, X_):\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        return pipeline.predict_proba(X_)[:, 1]\n",
    "    if hasattr(pipeline, \"decision_function\"):\n",
    "        return pipeline.decision_function(X_)\n",
    "    return None\n",
    "\n",
    "predictions_ethic = {}\n",
    "scores_ethic = {}\n",
    "\n",
    "for name, pipe in models_ethic.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions_ethic[name] = pipe.predict(X_test)\n",
    "    scores_ethic[name] = get_score(pipe, X_test)\n"
   ],
   "id": "f92dc5450b8d7022"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance evaluation after ethical feature removal\n",
    "\n",
    "We evaluate the models using metrics adapted to class imbalance:\n",
    "\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision / Recall / F1 (Attrition = 1)\n",
    "- ROC-AUC\n",
    "- PR-AUC (Average Precision)\n",
    "\n",
    "This allows us to assess the impact of ethical constraints on performance.\n"
   ],
   "id": "dc7d43c6c1e483e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 1) Build results_ethic (metrics)\n",
    "# ================================\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "def build_results(y_true, predictions, scores):\n",
    "    results = {}\n",
    "    for name in predictions.keys():\n",
    "        y_pred = predictions[name]\n",
    "        y_score = scores.get(name, None)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        rec  = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        f1v  = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        roc_auc = np.nan\n",
    "        pr_auc  = np.nan\n",
    "        if y_score is not None:\n",
    "            roc_auc = roc_auc_score(y_true, y_score)\n",
    "            pr_auc  = average_precision_score(y_true, y_score)\n",
    "\n",
    "        results[name] = {\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"Accuracy\": acc,\n",
    "            \"BalancedAcc\": bacc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1v,\n",
    "            \"ROC_AUC\": roc_auc,\n",
    "            \"PR_AUC\": pr_auc\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results_ethic = build_results(y_test, predictions_ethic, scores_ethic)\n",
    "\n",
    "list(results_ethic.keys())\n"
   ],
   "id": "1e50f07f305c80ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix display (ethical models)\n",
    "\n",
    "We display confusion matrices for each ethical model to observe:\n",
    "- False Positives (FP)\n",
    "- False Negatives (FN)\n",
    "- True Positives (TP)\n",
    "- True Negatives (TN)\n"
   ],
   "id": "f37a594e2dc16444"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 2) Confusion matrices (ethical)\n",
    "# ================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = [0, 1]\n",
    "    plt.xticks(tick_marks, [\"0 (Stay)\", \"1 (Leave)\"])\n",
    "    plt.yticks(tick_marks, [\"0 (Stay)\", \"1 (Leave)\"])\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for name, r in results_ethic.items():\n",
    "    plot_confusion_matrix(r[\"confusion_matrix\"], f\"Confusion Matrix — {name} (Ethical)\")\n"
   ],
   "id": "1921ef66e65eac87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ROC and Precision-Recall curves (ethical models)\n",
    "\n",
    "We plot:\n",
    "- ROC curves (with ROC-AUC)\n",
    "- Precision-Recall curves (with PR-AUC / Average Precision)\n",
    "\n",
    "PR curves are recommended for imbalanced classification.\n"
   ],
   "id": "d1764c54a5ba9176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 3) ROC + PR curves (ethical)\n",
    "# ================================\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "for name in models_ethic.keys():\n",
    "    y_score = scores_ethic.get(name, None)\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    auc = roc_auc_score(y_test, y_score)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Ethical models\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PR\n",
    "plt.figure(figsize=(6,5))\n",
    "for name in models_ethic.keys():\n",
    "    y_score = scores_ethic.get(name, None)\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_score)\n",
    "    ap = average_precision_score(y_test, y_score)\n",
    "    plt.plot(rec, prec, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve — Ethical models\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "fc98636c57946079"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Metrics table (ethical models)\n",
    "\n",
    "We aggregate metrics into a single table and sort by PR-AUC (recommended for imbalance).\n"
   ],
   "id": "93b6fcd07e44e8dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 4) Metrics table (ethical)\n",
    "# ================================\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for name, r in results_ethic.items():\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": r[\"Accuracy\"],\n",
    "        \"BalancedAccuracy\": r[\"BalancedAcc\"],\n",
    "        \"Precision(1)\": r[\"Precision\"],\n",
    "        \"Recall(1)\": r[\"Recall\"],\n",
    "        \"F1(1)\": r[\"F1\"],\n",
    "        \"ROC_AUC\": r[\"ROC_AUC\"],\n",
    "        \"PR_AUC\": r[\"PR_AUC\"]\n",
    "    })\n",
    "\n",
    "metrics_table_ethic = pd.DataFrame(rows).sort_values(by=\"PR_AUC\", ascending=False)\n",
    "metrics_table_ethic\n"
   ],
   "id": "29d371a03a2e9da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final comparison: ethical vs non-ethical\n",
    "\n",
    "We combine:\n",
    "- `metrics_table` (non-ethical baseline)\n",
    "- `metrics_table_ethic` (ethical filtered features)\n",
    "\n",
    "and compare them in one table.\n"
   ],
   "id": "879e52bc202738b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ================================\n",
    "# 5) Final comparison table\n",
    "# ================================\n",
    "metrics_table[\"Setup\"] = \"Non-ethical\"\n",
    "metrics_table_ethic[\"Setup\"] = \"Ethical\"\n",
    "\n",
    "final_comparison = pd.concat([metrics_table, metrics_table_ethic], ignore_index=True)\n",
    "\n",
    "final_comparison = final_comparison[\n",
    "    [\"Setup\", \"Model\", \"Accuracy\", \"BalancedAccuracy\",\n",
    "     \"Precision(1)\", \"Recall(1)\", \"F1(1)\", \"ROC_AUC\", \"PR_AUC\"]\n",
    "].sort_values(by=[\"Setup\", \"PR_AUC\"], ascending=[True, False])\n",
    "\n",
    "final_comparison\n"
   ],
   "id": "30feb98df170eef2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------------------------------------------------",
   "id": "327dded940ad108b"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
